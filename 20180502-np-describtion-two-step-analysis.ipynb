{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Description of the two-step analysis\n",
    "\n",
    "Author:<br>\n",
    "Nora Poel (1,2)<br>\n",
    "(1) CNRS/UGA/IRD/G-INP, IGE, Grenoble, France<br>\n",
    "(2) University of Potsdam, Institute for Computer Science, Potsdam, Germany\n",
    "\n",
    "The aim of this notebook is to describe the preparation and execution of the two-step analysis. The two-step analysis was developed specifically in preparation of the SWOT mission. It performs a data assimilation update with an observation vector that is augmented by **gradient observations** and with a corresponding observation error covariance matrix $\\mathbf{R^+}$. The analysis will be run with an implementation of an Ensemble Kalman Filter ([SESAM](http://pp.ige-grenoble.fr/pageperso/brankarj/SESAM/)).\n",
    "\n",
    "In the Ensemble Kalman Filter the pdf of the **state vector** is represented by an ensemble (sample) of members (states). In the two-step analysis the state vector consists of 6 variables all projected on different grids:<br>\n",
    "ssh provided by a numerical model on gridT<br>\n",
    "ssh projected with the SWOT simulator on gridSWOT<br>\n",
    "first derivation along track calculated on gridD1a<br>\n",
    "first derivation across track calculated on gridD1c<br>\n",
    "second derivation along track calculated on gridD2a<br>\n",
    "second derivation across track calculated on gridD2c<br>\n",
    "\n",
    "In a first step we generate an ensemble of numerical model outputs that is afterwards extended by the other variables. For the development of the analysis process and the data preparation tools, a dataset consisting of 474 outputs (with a frequency of one day) from the NATL60 simulation between june 2012 and october 2013 for the OSMOSIS region was provided. In the following, the dataset will be addressed as ```alldata.nc```.\n",
    "\n",
    "In a second step we generate the **observation vector** for the analysis based on the true state of the system. As true state we consider day 100 from the initial dataset. We then extend the observation vector with first and second derivations along and across track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generation of background information\n",
    "As background information for the analysis an ensemble is generated from alldata.nc using the bash script [make_summer_ensemble.ksh](data_prep/make_summer_ensemble.ksh).<br>\n",
    "The generated ensemble is named<br>\n",
    "\n",
    "`ENS0060.nc.bas`<br>\n",
    "\n",
    "and consists of 60 members, named as followed: <br>\n",
    "\n",
    "`vctgridT<nnnn>.nc`\n",
    "\n",
    "with `<nnnn>` ranging from 0001 to 0060. `gridT` describes the grid of the model output.<br>\n",
    "(**Note**: for naming see [SESAM](http://pp.ige-grenoble.fr/pageperso/brankarj/SESAM/) (--> SESAM data types) as there are restrictions for ensemble and member names from the analysis tool SESAM.)\n",
    "\n",
    "## 2. Generation of the true state\n",
    "As true state we consider day 100 from the initial dataset. It is extracted from `alldata.nc` using the bash command:<br>\n",
    "\n",
    "```ncks -d time_counter,100,100 alldata.nc xt_0060_gridT.nc```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Generation of SWOT-like observations\n",
    "### 3.1 Extend ensemble\n",
    "For each member of the ensemble `ENS0060.nc.bas` we use the SWOT simulator to project ssh on a SWOT-like grid.\n",
    "The SWOT simulator was created in preparation of the SWOT mission to simulate SWOT-like observations. To extend the ensemble we only need the projection of the ssh from the model grid on the SWOT-like grid (variable name in the file generated by the simulator: `ssh_model`). For the generation of the R-matrix in section 5 we also need the realization of the expected observation errors (variable name in the file generated by the simulator: `ssh_obs`). For this work, a single SWOT pass (pass 182) that crosses the OSMOSIS region from the northwest corner to the south (center) was chosen to represent the SWOT-like grid and observation. Normally, the simulator generates all passes that cross the input region. To generate only one single pass, we let the simulator generate (almost) all passes and corresponding grids for the first member of the ensemble (see step i - iii below) and then delete all grids except of the one of the desired pass. Following steps v and vi the simulator will then only produce a pass on the provided grid.<br>\n",
    "(**Note:** The gap between the two swaths is filled by the simulator.)\n",
    "\n",
    "i) parameter file for the simulator:<br>\n",
    "[params_pass182_nogap.py](data_prep/params_pass182_nogap.py)\n",
    "\n",
    "ii) prepare input directory (input/ENS0060.nc.bas); add [list_of_file.txt](data_prep/list_of_file.txt) and [list_tmp.txt](data_prep/list_tmp.txt) in the same directory\n",
    "- list_of_file.txt should consist of the following two lines:\n",
    "        vctgridT0001.nc\n",
    "        vctgridT0001.nc\n",
    "- list_tmp.txt should consist of the following two lines:\n",
    "        vctgridTNB.nc\n",
    "        vctgridTNB.nc\n",
    "        \n",
    "iii) create output directory (output/OBS0060)\n",
    "\n",
    "iv) run the simulator for the first member of the ensemble to generate the grid of pass182 (verify that `makegrid=TRUE`):<br> \n",
    "`swotsimulator params_pass182_nogap.py`\n",
    "\n",
    "v) delete all files except `swot292_grid_p182.nc` and `swot292_gridnadir_p182.nc` in output directory and change `makegrid` from TRUE to FALSE in `params_pass182_nogap.py` (in the following only pass 182 will be created)\n",
    "\n",
    "vi) run [create_obs_ens60.ksh](data_prep/create_obs_ens60.ksh) to call the simulator for each member of ensemble (except first member)\n",
    "\n",
    "### 3.2 Observations of x-true\n",
    "We create several observations from the true state to chose one of them for the subsequent analysis. Observations are created for the same pass (pass 182) as used to extend the ensemble. We can either create observations with the 20 km gap between the two swaths or without (gap filled by the simulator).\n",
    "\n",
    "i) **with gap** use [params_pass182_withgap.py](data_prep/params_pass182_withgap.py)<br>\n",
    "   **without gap** use [params_pass182_nogap.py](data_prep/params_pass182_nogap.py)\n",
    "   \n",
    "ii) prepare input directory (input/xt_0060); add true state (`xt_0060_gridT.nc`) and [list_of_file.txt](data_prep/obs/list_of_file.txt) to directory\n",
    "- list_of_file.txt should consist of the following two lines:\n",
    "        xt_0060_gridT.nc\n",
    "        xt_0060_gridT.nc\n",
    "\n",
    "iii) create output directory (output/p182_from_xt_0060); copy `swot292_grid_p182.nc` and `swot292_gridnadir_p182.nc` in output directory\n",
    "\n",
    "iv) adjust parameter file: `makegrid=FALSE` and set right input and output directory paths\n",
    "\n",
    "v) run [create_p182_from_xt_0060.ksh](data_prep/create_p182_from_xt_0060.ksh); adjust number of observations that will be produced and name of parameter file (with/without gap) in the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Processing SWOT simulator outputs\n",
    "\n",
    "### 4.1 Processing the ensemble\n",
    "\n",
    "i) for each members of the observation-ensemble (`OBS0060`, generated in 3.1) convert variables lon,lat,ssh_model,ssh_obs to real numbers using:<br>\n",
    "[convert_vct_to_realnb.ksh](data_prep/convert_vct_to_realnb.ksh)\n",
    "        \n",
    "ii) process the observation-ensemble (`OBS0060`) using [20180418-np-prepare-analysis.ipynb](20180418-np-prepare-analysis.ipynb) (section 1)\n",
    "- remove double nadir in from each member of the observation-ensemble (`OBS0060`)\n",
    "- denoise ssh_obs to later generate R and\n",
    "- augment ssh_model with its derivatives\n",
    "\n",
    "\n",
    "### 4.2 Processing an observation\n",
    "\n",
    "i) chose one observation simulated from x-true (here we chose: `obs_p182_SWOT0004.nc`)\n",
    "        \n",
    "ii) convert ssh_obs,lon,lat to real numbers: (probably not necessary because SWOTdenoise rewrite the data with real numbers)\n",
    "\n",
    "        ncap -O -s \"ssh_obs=(ssh_obs*1.0);lon=(lon*1.0);lat=(lat*1.0)\" obs_p182_SWOT0004.nc obs_p182_SWOT0004.nc\n",
    "     \n",
    "iii) process `obs_p182_SWOT0004.nc` using [20180418-np-prepare-analysis.ipynb](20180418-np-prepare-analysis.ipynb) (section 2)\n",
    "- remove the double nadir,\n",
    "- denoise observation and \n",
    "- extend the observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generation of R\n",
    "\n",
    "i) estimate parameters for the R-matrix and generate a R-matrix for each (gradient) observation using [20180418-np-parametrization-R.ipynb](20180418-np-parametrization-R.ipynb).\n",
    "\n",
    "\n",
    "## 6. Perform the two-step analysis\n",
    "We need to split the analysis in two subsequent analysis. This is, because there is no implementation in SESAM to interpolate gradient observations on the model grid (gridT) (= there is no implementation of the observation operator $\\mathbf{H}$ for gradient observations). In the first analysis we are using all five (gradient) observations (ssh, first and second derivation along and across track) and their corresponding R matrixes to perform an update on each grid. The update that is performed on gridT shows an updated area but in a region that is spacially not correlated with the location of the observation. The update that is performed on the observation grid (gridSWOT) is spatially not correlated to the model grid but contains the updated ssh. With a second analysis we then project the updated ssh from gridSWOT on the model grid. We thereby assign a low standard deviation to the observation (here: 0.0001) so that the observation has a high certainty in the analysis. \n",
    "\n",
    "### 6.1 First analysis\n",
    "#### 6.1.1 Prepare the analysis directory\n",
    "The directory for the first analysis should contain:\n",
    "\n",
    "i) the extended `ENS0060.nc.bas`; unite background-ensemble (`ENS0060.nc.bas` containing gridT) and processed observation-ensemble (`OBS0060` containing gridSWOT, gridD1a, gridD1c, gridD2a, gridD2c) in `ENS0060.nc.bas`\n",
    "\n",
    "ii) all (gradient) observations; the file type need to be changed from `.nc` to `.ncdta` (e.g. `mv rmdc_obs_p182_SWOT0004_denoised_D1a.nc rmdc_obs_p182_SWOT0004_denoised_D1a.ncdta`), here:<br>\n",
    "\n",
    "   `rmdc_obs_p182_SWOT0004_denoised_D1a.ncdta`<br>\n",
    "    `rmdc_obs_p182_SWOT0004_denoised_D1c.ncdta`<br>\n",
    "    `rmdc_obs_p182_SWOT0004_denoised_D2a.ncdta`<br>\n",
    "    `rmdc_obs_p182_SWOT0004_denoised_D2c.ncdta`<br>\n",
    "    `rmdc_obs_p182_SWOT0004_denoised_SWOT.ncdta`\n",
    "    \n",
    "iii) five R-matrixes:<br>\n",
    "\n",
    "   `R_gridSWOT.nc`<br>\n",
    "    `R_gridD1a.nc`<br>\n",
    "    `R_gridD1c.nc`<br>\n",
    "    `R_gridD2a.nc`<br>\n",
    "    `R_gridD2a.nc`\n",
    "    \n",
    "iv) one mask for each grid type, create e.g. as followed:<br>\n",
    "\n",
    "cp ENS0060.nc.bas/vctgridSWOT0001.nc mask_gridT.nc<br>\n",
    "cp obs_p182_SWOT0007_denoised_SWOT.ncdta mask_gridSWOT.nc<br>\n",
    "cp obs_p182_SWOT0007_denoised_D1a.ncdta mask_gridD1a.nc<br>\n",
    "cp obs_p182_SWOT0007_denoised_D1c.ncdta mask_gridD1c.nc<br>\n",
    "cp obs_p182_SWOT0007_denoised_D2a.ncdta mask_gridD2a.nc<br>\n",
    "cp obs_p182_SWOT0007_denoised_D2c.ncdta mask_gridD2c.nc\n",
    "\n",
    "(**Note**: Masks need to consist the same variable as the corresponding data files. All (gradient) observation masks must be converted to real numbers. The $\\_$FillValue must be denoted in the masks. This is the case if they are produced as above.)\n",
    "\n",
    "v) a domain localization configuration; here [L10localization.cfg](data_prep/L10localization.cfg), consisting of 4 lines:<br>\n",
    "1 1 <br>\n",
    "30 30<br>\n",
    "10 10<br>\n",
    "end<br>\n",
    "\n",
    "vi) the configuration file for SESAM [sesamlist1](data_prep/sesamlist1)<br>\n",
    "(**Note**: to be renamed as \"sesamlist\")\n",
    "\n",
    "#### 6.1.2 Run analysis 1\n",
    "To run the analysis use [run_analysis1_L10_p182_0060.ksh](data_prep/run_analysis1_L10_p182_0060.ksh).<br>\n",
    "The script will perform the following steps:<br>\n",
    "1. remove previous localization files if exist and create localization files\n",
    "2. remove previous analysis results and create an output folder for the analysis\n",
    "3. run the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6.2 Second Analysis\n",
    "#### 6.2.1 Prepare the analysis directory\n",
    "The directory of the second analysis should contain:\n",
    "\n",
    "i) `ENS0060.nc.bas` containing only gridT\n",
    "\n",
    "ii) the result of analysis 1 on gridSWOT (here: xa_ana1_gridSWOT.nc)\n",
    "\n",
    "iii) mask_gridT.nc from analysis 1 directory\n",
    "\n",
    "iv) the configuration file for SESAM [sesamlist2](data_prep/sesamlist2)<br>\n",
    "(**Note**: to be renamed as sesamlist)\n",
    "\n",
    "#### 6.2.2 Run analysis 2\n",
    "To run the analysis use [run_analysis2_L10_p182_0060.ksh](data_prep/run_analysis2_L10_p182_0060.ksh).<br>\n",
    "The script will perform the following steps:<br>\n",
    "1. generation of R for gridT using mask_gridT.nc as input\n",
    "2. rename the observation\n",
    "3. rename the mask according the specifications in sesamlist\n",
    "4. adjust the longitude of the observation: The output of the first analysis stores longitude from 0째 to 360째 whereas in gridT files (background model) longitude ranges uses the notation from 0째 to 180째 with a minus sign in eastward and plus sign is westward direction. The scale of the result of analysis 1 will be adapted by the script.\n",
    "5. perform the observation management\n",
    "6. remove previous localization files if exist and create localization files\n",
    "7. remove previous analysis results and create an output folder for the analysis\n",
    "8. run the analysis\n",
    "\n",
    "## 7. Additional SESAM commands\n",
    "\n",
    "i) Compute the ensemble mean and standard deviation (for all grids in ensemble)<br>\n",
    "`sesam -mode oper -incfg list_ENS0060.cfg -outvar xf_#.nc -typeoper mean`<br>\n",
    "`sesam -mode oper -incfg list_ENS0060.cfg -outvar stdf_#.nc -typeoper std`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
